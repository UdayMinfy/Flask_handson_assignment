{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e44fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/02 21:41:53 INFO mlflow.tracking.fluent: Experiment with name 'Data Drift Analysis' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged run: Train_vs_Test_Historical, Report: report_train_vs_test.html\n",
      "✅ Logged run: HistoricalCSV_vs_NewDataCSV, Report: report_historical_vs_new.html\n",
      "\n",
      "✅ Both runs completed and logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, DataQualityPreset, TargetDriftPreset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === CONFIG ===\n",
    "#HISTORICAL_DATA_PATH = \"historical_data.csv\"\n",
    "NEW_DATA_PATH = \"New Customer Bank_Personal_Loan.csv\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"Data Drift Analysis\"\n",
    "\n",
    "# === Load Data ===\n",
    "historical_df = pd.read_excel(\"Bank_Personal_Loan_Modelling.xlsx\",sheet_name='Data')\n",
    "new_df = pd.read_csv(NEW_DATA_PATH)\n",
    "\n",
    "# Drop ID or non-feature columns if needed\n",
    "if 'ID' in historical_df.columns:\n",
    "    historical_df = historical_df.drop(columns=['ID'])\n",
    "if 'ID' in new_df.columns:\n",
    "    new_df = new_df.drop(columns=['ID'])\n",
    "\n",
    "# Separate features and target if target column exists\n",
    "target_column = 'Personal Loan' if 'Personal Loan' in historical_df.columns else None\n",
    "if target_column:\n",
    "    X = historical_df.drop(columns=[target_column])\n",
    "    y = historical_df[target_column]\n",
    "else:\n",
    "    X = historical_df\n",
    "    y = None\n",
    "\n",
    "# Split for train/test (only for Run 1)\n",
    "train_df, test_df = train_test_split(X, test_size=0.3, random_state=42)\n",
    "\n",
    "# === Setup MLflow ===\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# === Function to Run Report, Save, Log to MLflow ===\n",
    "def log_data_drift_run(run_name, reference_df, current_df, report_file):\n",
    "    report = Report(metrics=[\n",
    "        DataQualityPreset(),\n",
    "        DataDriftPreset(),\n",
    "        TargetDriftPreset()\n",
    "    ])\n",
    "    report.run(reference_data=reference_df, current_data=current_df)\n",
    "    report.save_html(report_file)\n",
    "    report_dict = report.as_dict()\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log column-level drift scores\n",
    "        drift_results = report_dict.get(\"metrics\", [])\n",
    "        for metric in drift_results:\n",
    "            if metric.get(\"metric\") == \"DataDriftTable\":\n",
    "                for feature in metric.get(\"result\", {}).get(\"drift_by_columns\", {}):\n",
    "                    score = metric[\"result\"][\"drift_by_columns\"][feature].get(\"drift_score\")\n",
    "                    stat_test = metric[\"result\"][\"drift_by_columns\"][feature].get(\"stat_test_name\")\n",
    "                    detected = metric[\"result\"][\"drift_by_columns\"][feature].get(\"drift_detected\")\n",
    "                    mlflow.log_metric(f\"{feature}_drift_score\", score)\n",
    "                    mlflow.log_param(f\"{feature}_stat_test\", stat_test)\n",
    "                    mlflow.log_param(f\"{feature}_drift_detected\", detected)\n",
    "        # Log HTML report\n",
    "        mlflow.log_artifact(report_file, artifact_path=\"evidently_report\")\n",
    "        print(f\"✅ Logged run: {run_name}, Report: {report_file}\")\n",
    "\n",
    "# === Run 1: Train vs Test on Historical CSV ===\n",
    "log_data_drift_run(\n",
    "    run_name=\"Train_vs_Test_Historical\",\n",
    "    reference_df=train_df,\n",
    "    current_df=test_df,\n",
    "    report_file=\"report_train_vs_test.html\"\n",
    ")\n",
    "\n",
    "# === Run 2: Historical CSV (full) vs New Data CSV ===\n",
    "log_data_drift_run(\n",
    "    run_name=\"HistoricalCSV_vs_NewDataCSV\",\n",
    "    reference_df=X,        # full historical features\n",
    "    current_df=new_df,\n",
    "    report_file=\"report_historical_vs_new.html\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Both runs completed and logged to MLflow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb99d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c0faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bankloan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
